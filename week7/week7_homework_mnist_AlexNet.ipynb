{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业：\n",
    "1. 完成week7/week7_homework_mnist_alexnet.py代码的填空，主要为熟悉alexnet的网络结构，并掌握flops和参数量的计算方法。\n",
    "2. [可选]使用voc2005数据集，用alexnet对其进行分类，重点尝试复现其结果。数据增强，dropout等。\n",
    "3. voc2005下载地址：http://host.robots.ox.ac.uk/pascal/VOC/databases.html#VOC2005_1\n",
    "\n",
    "#### 要求：\n",
    "1. 实现alexnet网络的正向计算过程\n",
    "2. 实现参数量计算函数:print_params_num\n",
    "3. 实现计算量flops计算函数:get_flops\n",
    "\n",
    "__步骤__：\n",
    "###### 对week7/week7_homework_mnist_alexnet.py填空\n",
    "1. 184-209行 完成alexnet网络结构的构建\n",
    "2. 162行完成一个层参数量的计算，注意区分卷积层与全连接层\n",
    "3. 25行完成一个层的flops计算。\n",
    "5. 运行办法：python week7/week7_homework_mnist_alexnet.py 0.00001\n",
    "##### 其它参考材料：\n",
    "1. week7/初探alexnet网络结构.pdf\n",
    "2. week7/Alexnet-imagenet2012.pdf\n",
    "3. conv2d的参数意义以及写法参考：week6/conv.py\n",
    "4. conv,pool 函数说明：week7/pytorch_api_conv_pool.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import  nn\n",
    "from itertools import product\n",
    "import sys\n",
    "# from mnist import MNIST\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../../week4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完成对一个层的flops计算\n",
    "def get_flops(layer,fea):\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    # 这里填入flops计算方法：\n",
    "    flops =\n",
    "    flops = float(flops)\n",
    "    print_f(layer)\n",
    "    print_f(\"flops=%s / %.2f M / %.2f G \"%(flops,flops/(1024.**2),flops/(1024.**3)))\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完成对一个层得计算时间估计和flops的计算\n",
    "def forword_flops(layer,fea): \n",
    "    start_time = time.clock()\n",
    "    fea= torch.relu(layer(fea))\n",
    "    end_time = time.clock()\n",
    "    flops=get_flops(layer,fea)\n",
    "    time_cost=end_time-start_time\n",
    "    print_f(\"time cost:%s S,computer flops:%s \"%(time_cost,flops/(1024.**3)/time_cost))\n",
    "    return fea,time_cost,flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(fea,flag=\"train\"):\n",
    "    pdb()\n",
    "    if flag==\"train\":\n",
    "        size= fea.shape\n",
    "        a = torch.empty(size[0],size[1]).uniform_(0, 1)\n",
    "        p=torch.bernoulli(a)\n",
    "        fea=fea*p\n",
    "    elif flag==\"evluate\":\n",
    "        fea=fea*0.5\n",
    "    return fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(feature,layers):\n",
    "    y=-1\n",
    "    # time cost sum\n",
    "    tcs=0\n",
    "    # flops sum\n",
    "    fls=0\n",
    "    B = len(feature)\n",
    "    fea=torch.tensor(feature).view(B,1,28,28).float()\n",
    "    #放大到alexnet需要的尺寸\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    fea = nn.functional.interpolate(fea,(224,224),mode='nearest')\n",
    "    #fea = nn.functional.upsample_bilinear(fea, (224,224))\n",
    "    #fea= torch.rand(100,3,224,224)\n",
    "    fea=torch.cat([fea,fea,fea],1)\n",
    "    B = fea.shape[0]\n",
    "    print_f(\"feature map size:[%s,%s,%s,%s]\"%(fea.shape))\n",
    "    start_time = time.clock()\n",
    "    fea= torch.relu(layers[0](fea))\n",
    "    end_time = time.clock()\n",
    "    flops=get_flops(layers[0],fea)\n",
    "    time_cost=end_time-start_time\n",
    "    print_f(\"time cost:%s S\"%(end_time-start_time))\n",
    "    tcs+=time_cost\n",
    "    fls+=flops\n",
    "    \n",
    "    fea= layers[1](fea)\n",
    "    #fea= torch.relu(layers[2](fea))\n",
    "    fea,tc,fl=forword_flops(layers[2],fea)\n",
    "    tcs+=tc\n",
    "    fls+=fl\n",
    "\n",
    "    fea= layers[3](fea)\n",
    "    #fea= torch.relu(layers[4](fea))\n",
    "    fea,tc,fl=forword_flops(layers[4],fea)\n",
    "    tcs+=tc\n",
    "    fls+=fl\n",
    "    #fea= torch.relu(layers[5](fea))\n",
    "    fea,tc,fl=forword_flops(layers[5],fea)\n",
    "    tcs+=tc\n",
    "    fls+=fl\n",
    "    #fea= torch.relu(layers[6](fea))\n",
    "    fea,tc,fl=forword_flops(layers[6],fea)\n",
    "    tcs+=tc\n",
    "    fls+=fl\n",
    "    print_f(\"sum_time_cost:%s,sum_flops:%s,computer_flops:%s\"%(tcs,fls,fls/tcs/(1024.**3)))\n",
    "    fea= layers[7](fea)\n",
    "    fea = fea.view(B,9216)\n",
    "    fea= torch.relu(layers[8](fea))\n",
    "    fea=dropout(fea)\n",
    "    \n",
    "    fea= torch.relu(layers[9](fea))\n",
    "    fea=dropout(fea)\n",
    "    output= torch.sigmoid(dropout(layers[10](fea)))\n",
    "    y=output\n",
    "    #pdb()\n",
    "    #y=torch.softmax(output,1)\n",
    "    #y = 1.0/(1.0+torch.exp(-1.*h))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(image_data,image_label,layers,start_i,end_i):\n",
    "    correct=0\n",
    "    for i in range(start_i,end_i):\n",
    "        y = model(image_data[i:i+1],layers)\n",
    "        gt = image_label[i]\n",
    "        pred = torch.argmax(y).item()\n",
    "        if gt==pred:\n",
    "            correct+=1\n",
    "    #print(\"acc=%s\"%(float(correct/20.0)))\n",
    "    return  float(correct/float(end_i-start_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(image_data,image_label,layers,lr):\n",
    "    loss_value_before=1000000000000000.\n",
    "    loss_value=10000000000000.\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    for epoch in range(0,300):\n",
    "        loss_value_before=loss_value\n",
    "        loss_value=0\n",
    "        #print(image_label[i])\n",
    "        B = len(image_data)\n",
    "        B = 80\n",
    "        y = model(image_data[0:B],layers)\n",
    "        gt=torch.tensor(image_label[0:B]).view(B,1)\n",
    "        # get one_hot\n",
    "        gt_vector = torch.zeros(B,1000).scatter_(1,gt,1)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        # 关心所有值\n",
    "        loss = torch.sum((y-gt_vector).mul(y-gt_vector))\n",
    "        # 优化loss，正样本接近1，负样本远离1\n",
    "        #loss1 = (y-1.0).mul(y-1.0)\n",
    "        #loss = loss1[0,gt]+torch.sum(1.0/(loss1[0,0:gt]))+torch.sum(1.0/(loss1[0,gt:-1]))\n",
    "        loss_value += loss.data.item()\n",
    "        # 更新公式\n",
    "        # w  = w - (y-y1)*x*lr\n",
    "        loss.backward()\n",
    "        for i in [0,2,4,5,6,8,9,10]: \n",
    "            layers[i].weight.data.sub_(layers[i].weight.grad.data*lr)\n",
    "            layers[i].weight.grad.data.zero_()\n",
    "            layers[i].bias.data.sub_(layers[i].bias.grad.data*lr)\n",
    "            layers[i].bias.grad.data.zero_()\n",
    "        train_acc=get_acc(image_data,image_label,layers,0,80)\n",
    "        test_acc =get_acc(image_data,image_label,layers,80,100)\n",
    "        print(\"epoch=%s,loss=%s/%s,train/test_acc:%s/%s\"%(epoch,loss_value,loss_value_before,train_acc,test_acc))\n",
    "    return layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week7作业：完善并打印网络各层得参数量\n",
    "def print_params_num(layers):\n",
    "    print(20*\"*\")\n",
    "    params_num=0\n",
    "    params_num_K=0\n",
    "    params_num_M=0\n",
    "    for i in [0,2,4,5,6,8,9,10]: \n",
    "        ### 这里填入每层参数量的计算方式\n",
    "        layer_num = \n",
    "        # 换算为k\n",
    "        layer_num_K = layer_num/1024.\n",
    "        # 换算为M\n",
    "        layer_num_M = layer_num_K/1024.\n",
    "        print(layers[i])\n",
    "        print(\"layer[%s] has %s / %sK / %sM params\"%(i,layer_num,layer_num_K,layer_num_M))\n",
    "        params_num +=layer_num\n",
    "        params_num_K +=layer_num_K\n",
    "        params_num_M +=layer_num_M\n",
    "    print(\"alexnet has %s / %sK / %sM params need to train\"%(params_num,params_num_K,params_num_M))\n",
    "    print(20*\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # 从输入中获取学习率\n",
    "    lr = float(sys.argv[1])\n",
    "    \n",
    "    layers=[]\n",
    "    # 完善alexnet的网络结构，填入其需要得参数\n",
    "    # add conv1 \n",
    "    # 填写输入，输出通道数\n",
    "    conv1=nn.Conv2d( , ,kernel_size = 11,stride=4,padding=2)\n",
    "    layers.append(conv1)\n",
    "    # 填写kernel_size 和stride\n",
    "    pool2=nn.MaxPool2d(kernel_size= , stride= , padding=0,ceil_mode=True)\n",
    "    layers.append(pool2)\n",
    "    # add conv3 \n",
    "    # 填写输入，输出通道数\n",
    "    conv3=nn.Conv2d(  ,  ,kernel_size = 5,stride=1,padding=2)\n",
    "    layers.append(conv3)\n",
    "    pool4=nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    layers.append(pool4)\n",
    "    # add conv5 \n",
    "    # 填写输入，输出通道数\n",
    "    conv5=nn.Conv2d( , ,kernel_size = 3,stride=1,padding=1)\n",
    "    layers.append(conv5)\n",
    "    # 填写输入，输出通道数\n",
    "    conv6=nn.Conv2d( , ,kernel_size = 3,stride=1,padding=1)\n",
    "    layers.append(conv6)\n",
    "    # 填写输入，输出通道数\n",
    "    conv7=nn.Conv2d( , ,kernel_size = 3,stride=1,padding=1)\n",
    "    layers.append(conv7)\n",
    "    # 填写kernel_size 和stride\n",
    "    pool8=nn.MaxPool2d(kernel_size= , stride= , padding=0)\n",
    "    layers.append(pool8)\n",
    "    # 填写输入，输出神经元数\n",
    "    fc9 = nn.Linear( , )\n",
    "    layers.append(fc9)\n",
    "    fc10 = nn.Linear(4096, 4096)\n",
    "    layers.append(fc10)\n",
    "    fc11 = nn.Linear(4096, 1000)\n",
    "    layers.append(fc11)\n",
    "    #打印出往略得参数量\n",
    "    print_params_num(layers)\n",
    "    # 记载数据\n",
    "    # minst 2828 dataset 60000 samples\n",
    "    mndata = MNIST('../week4/mnist/python-mnist/data/')\n",
    "    image_data_all, image_label_all = mndata.load_training()\n",
    "    image_data=image_data_all[0:100]\n",
    "    image_label=image_label_all[0:100]\n",
    "    # 使用未训练的模型处理数据\n",
    "    y = model(image_data,layers)\n",
    "    pdb()\n",
    "    # 使用为训练得模型测试 \n",
    "    print(\"初始的未训练时模型的acc=%s\"%(get_acc(image_data,image_label,layers,80,100)))\n",
    "    pdb()\n",
    "    # 对模型进行训练：\n",
    "    train_model(image_data,image_label,layers,lr)\n",
    "    # 训练完成，对模型进行测试，给出测试结果：\n",
    "    print(\"训练完成后模型的acc=%s\"%(get_acc(image_data,image_label,layers,80,100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-AI",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
